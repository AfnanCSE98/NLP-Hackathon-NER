# -*- coding: utf-8 -*-
"""test_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OQpL6h20qPQbT3wRvY5mXCgkYotVEPhz
"""

# !wget --header="Host: doc-0s-54-docs.googleusercontent.com" --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9" --header="Accept-Language: en-US,en;q=0.9" --header="Cookie: AUTH_prqi0ddebcnouinrgq4n8k579qees3ri=09250325709264443050|1674244950000|1c1ogi62gqa0o073nvbr2kl2f6fikp72" --header="Connection: keep-alive" "https://doc-0s-54-docs.googleusercontent.com/docs/securesc/5n63bj25lj97nngplfo91cqd1qhl5kn0/8ej607icfh0m6j3fb5tj6udnh0la5itf/1674245025000/06977169721700109230/09250325709264443050/1z2lFY4SWkYU82vMtX1nYaMQZwU0a4HvR?e=download&ax=ALjR8sxbmW2AJUtlvX8dw48uRbOUdNPAYG91QPtqFSN38R7BL3ClSGdI95QBmCoOWDZBbub-fCLCrKdUtwrWWqTJjWzsCw28oxWDeF1ad7L3L-Dn_NpnD7EVG8hRCVjjyHhg5kczrEjIhUGJFgkCTXSdFPLALYTFvRHD7GEWxJul0QXLUm2hrMJjIhCQKkmLX39B6iO-HTEXRbw0OgPLZqkpMVwZyrtpWecRAir_yYqkulr2sPZ21TS30XoazMqRU7yfsKfqLLeMmGyc4O6Qa1bxBhp4nhwE6EjlenB1MDnN3zEPMctoUq6tCBvezYC6nQme-VIbhTdLDhKvBHp_VcDZ23KHDp_UlvPMPQkIBmXS_ihZRHyv2c1N_Zq5SJ2Z8iRJuNQoFYMTQbZNQA657HEa30uIupKQVDBPmb1k1Hgm5lkPfUP9zaWKWd77i4fd7ZyG1fnRamXiHYyHV2Kb9XXJt1KvBgKbv72aX_qDWwbRF1Xiy_lmGMYQ68GE0pRSRmhDnu0j8wllqYS7gbHuWuNqRFyHz_d4otycw0kMvVSV_dQkHcU04y1_tCIwBfL2xZixJJ-0qIE8ns-sTCLDPb0C_tNxufUDrQoYohqdyOHLfpLPrH9TDgtcUahBRj7TpFrP4MwuaILpYF-Pjbj5ifg1e8xyjdFODzlHTg0RWeqeaGWjGIh22OidVACNEFPwCClrZOBU7qx3W5Sn8VSBBWFLtSC3mGSoKCaryqmiGmeoqTwaAwz5_2rhaB-s-NjdLDXpc88cyqNhVsgueMVfFfdf_BK1shiqV_UGlsg1teXgPXeMSm03HxU1kWaT01_Qnm7lRT8WdZNXd8ndFObio_BAY5y8CEydet7gsoRd4cHTIuqs7EIB02faHSJIKkvVBqXY3ZtPGgkc3hjKGShU1ta-UfS2hEE4Sx_9NCjBYvoEJHuESI_l8PaSFY839dS8jKc0oM0umPPLS4SgkX4ORyL4DtkYmHVv0luOMSc&uuid=66b0f1f1-2bd2-4c63-8fba-60123d3f2d1c&authuser=2" -c -O 'csebuetnlp-banglabert_large.h5'

import numpy as np
import pandas as pd

# !pip install transformers
from transformers import AutoTokenizer

from tqdm import tqdm
import tensorflow as tf

import matplotlib.pyplot as plt

def format(file):
  with open(file) as f:
      # read csv file as pandas dataframe
      df = pd.read_csv(f, sep=',', 
                       # names=['Sentence_id', 'Word', 'Tag']
                      )
      # group by sentence_id and create list of words and list of tags
      df = df.groupby('sentence_id').agg({'word': lambda x: list(x), 'tag': lambda x: list(x)})
      # create pair of 2nd and 3rd column (word , tag)
      df['word_tag'] = df.apply(lambda x: list(zip(x['word'], x['tag'])), axis=1)
      # create list of pair of 2nd and 3rd column (word , tag)
      word_tag_list = df['word_tag'].tolist()
      
      return word_tag_list

file = 'data/train_generic.csv'
samples = format(file)

print(samples[:5])

schema = ['_'] + sorted({tag for sentence in samples 
                             for _, tag in sentence})

print(schema)

from transformers import AutoConfig, TFAutoModelForTokenClassification , AutoTokenizer

#MODEL_NAME = 'bert-base-cased' 
# MODEL_NAME = "csebuetnlp/banglabert"
MODEL_NAME = "csebuetnlp/banglabert_large"


config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema)) 

#old_model = 'csebuetnlp-banglabert0.6987703481430697.h5'
# old_model = 'data/csebuetnlp-banglabert_large_f1_81.h5'
old_model = "/mnt/y/_data/nlp23/strat/bertlarge20-40/csebuetnlp-banglabert_large.h5"
model = TFAutoModelForTokenClassification.from_pretrained(old_model, from_pt=False,
                                                       config=config)
                                          
optimizer = tf.keras.optimizers.Adam(lr=0.000001)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=optimizer, loss=loss, 
              metrics=["accuracy"])

model.summary()

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def format_test(file):
  with open(file) as f:
      # read csv file as pandas dataframe
      df = pd.read_csv(f, sep=',', 
                       # names=['Sentence_id', 'Word', 'Tag']
                      )
      # group by sentence_id and create list of words and list of tags
      df = df.groupby('sentence_id').agg({'word': lambda x: list(x)})
      # create pair of 2nd and 3rd column (word , tag)
      # df['word_tag'] = df.apply(lambda x: list(zip(x['word'], x['tag'])), axis=1)
      # create list of pair of 2nd and 3rd column (word , tag)
      word_list = df['word'].tolist()
      
      return word_list
subtoken_map = {}
def tokenize_sample(sample):
    # seq = [
    #            (subtoken, tag)
    #            for token, tag in sample
    #            for subtoken in tokenizer(token)['input_ids'][1:-1]
    #        ]
    seq = []
    sub_lengths = []
               # (subtoken, tag)
    for token in sample:
        subtokens = tokenizer(token)['input_ids'][1:-1]
        # subtoken_map
        for subtoken in subtokens:
            seq.append((subtoken))
        sub_lengths.append(len(subtokens))
           
    return [(3)] + seq + [(4)], sub_lengths 

def preprocess(samples):
    # tag_index = {tag: i for i, tag in enumerate(schema)}
    # tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
    tokenized_samples = []
    all_sub_lengths = []
    for s in tqdm(samples):
        seq, lengths = tokenize_sample(s)
        tokenized_samples.append(seq)
        all_sub_lengths.append(lengths)
        
        # print(lengths, sum(lengths))
        # break
        
    max_len = max(map(len, tokenized_samples))
    print(max_len)
    X = np.zeros((len(samples), max_len), dtype=np.int32)
    for i, sentence in enumerate(tokenized_samples):
        for j, (subtoken_id) in enumerate(sentence):
            X[i, j] = subtoken_id
        
    return X, all_sub_lengths

# file = 'sample_test_set_no_punc.csv'
file = 'data/test_generic.csv'
dev_samples = format_test(file)
X_valid, all_sub_lengths = preprocess(dev_samples)

# [loss, accuracy] = model.evaluate(X_valid, y_valid)
# print("Loss:%1.3f, Accuracy:%1.3f" % (loss, accuracy))

# Find the f1 score of the model output
from sklearn.metrics import f1_score
y_pred = model.predict(X_valid).logits
y_pred_raw = np.argmax(y_pred, axis=-1)
print(type(y_pred_raw))

y_pred_raw.shape

reverse_tag_index = {i:tag for i, tag in enumerate(schema)}
print(reverse_tag_index)
lens = []
for s in dev_samples:
    lens.append(len(s))
    
# print(lens)
print(max(lens))
cum_len = list(np.cumsum(lens))
# print(cum_len)

dev_pred_out = open('dev_pred_ours.txt', 'w')
# y_pred = list(filter(lambda a: a!= 0, y_pred) )
# print(y_pred[0])

labels = []

for i in range(y_pred_raw.shape[0]):
    # print(y , reverse_tag_index[y])
    index = 1
    for sublen in all_sub_lengths[i]:
        # print(reverse_tag_index[y_pred_raw[i][index]], file=dev_pred_out)
        label = reverse_tag_index[y_pred_raw[i][index]]
        labels.append(label)
        
        index += sublen
        
    # for j in range(1, lens[i] + 1):
    #     # print(y_pred_raw[i, j])
    #     print(reverse_tag_index[y_pred_raw[i][j]], file=dev_pred_out)
        
    # if (i + 1) in cum_len:
    labels.append('')
    
    # print(file=dev_pred_out)
    # break

dev_pred_out.close()

# for i, label in enumerate(labels):
#     if i + 1 < len(labels):
#         if len(labels[i]) > 0 and len(labels[i+1]) > 0:
#             if labels[i][0] == "B" and labels[i+1][0] == "B":
#                 labels[i+1][0] == "I"

dev_pred_out = open('dev_pred_ours.txt', 'w')
for i, label in enumerate(labels):
    print(label, file=dev_pred_out)

dev_pred_out.close()



# !python eval_script/eval_main.py --true eval_script/dev_gt_labels.txt --pred dev_pred_ours.txt

